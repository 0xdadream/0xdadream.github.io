{
    "version": "https://jsonfeed.org/version/1",
    "title": "逐梦 • All posts by \"ai\" tag",
    "description": "Welcome to my blog",
    "home_page_url": "https://0xdadream.github.io",
    "items": [
        {
            "id": "https://0xdadream.github.io/2025/06/26/arc-a770-ollama-webui-jiao-cheng/",
            "url": "https://0xdadream.github.io/2025/06/26/arc-a770-ollama-webui-jiao-cheng/",
            "title": "Arc A770 Ollama WebUI 教程",
            "date_published": "2025-06-26T04:25:00.000Z",
            "content_html": "<h1 id=\"Arc-A770-Ollama-WebUI-教程\"><a href=\"#Arc-A770-Ollama-WebUI-教程\" class=\"headerlink\" title=\"Arc A770 Ollama WebUI 教程\"></a>Arc A770 Ollama WebUI 教程</h1><p>在 Windows 上利用 Arc A770 16G 和 IPEX-LLM 运行 Ollama 大语言模型的终极指南</p>\n<h2 id=\"第一部分：简介-为本地-AI-释放您的-Arc-A770-显卡潜能\"><a href=\"#第一部分：简介-为本地-AI-释放您的-Arc-A770-显卡潜能\" class=\"headerlink\" title=\"第一部分：简介 - 为本地 AI 释放您的 Arc A770 显卡潜能\"></a>第一部分：简介 - 为本地 AI 释放您的 Arc A770 显卡潜能</h2><h3 id=\"1-1-前景与挑战\"><a href=\"#1-1-前景与挑战\" class=\"headerlink\" title=\"1.1. 前景与挑战\"></a>1.1. 前景与挑战</h3><p>英特尔锐炫 Arc A770 16G 显卡凭借其高达 16 GB 的海量显存，为在本地运行大型语言模型（LLM）提供了一个极具性价比和吸引力的硬件平台。充足的显存意味着可以流畅运行更大、更复杂的模型，这对于追求高质量本地 AI 体验的用户而言至关重要。</p>\n<p>然而，一个核心挑战阻碍了这条道路：标准的、从官方网站下载的 Windows 版 Ollama 应用程序，其默认配置并不能直接利用英特尔 Arc GPU 进行硬件加速 。这是因为其标准后端主要依赖于为英伟达（NVIDIA）设计的 CUDA 技术，或是为其他硬件设计的 DirectML 技术，而这些技术在标准 Ollama 的框架内并未对英特尔的 Xe HPG 架构提供原生支持 。直接安装标准版 Ollama 会导致模型推理任务完全由 CPU 执行，无法发挥 Arc A770 强大的并行计算能力。  </p>\n<h3 id=\"1-2-解决方案：英特尔-IPEX-LLM-这座桥梁\"><a href=\"#1-2-解决方案：英特尔-IPEX-LLM-这座桥梁\" class=\"headerlink\" title=\"1.2. 解决方案：英特尔 IPEX-LLM 这座桥梁\"></a>1.2. 解决方案：英特尔 IPEX-LLM 这座桥梁</h3><p>解决这一挑战的关键在于英特尔官方推出的“英特尔 PyTorch LLM 扩展库”（Intel® Extension for PyTorch* for LLM），简称 IPEX-LLM。它并非一个简单的插件，而是一个全面的加速库，为 Ollama 等框架提供了一个基于 oneAPI 和 SYCL 编程模型构建的、为英特尔 GPU 量身定制的后端 。  </p>\n<p>本指南的核心，正是利用一个由英特尔提供、经过特殊编译的 Ollama 版本。该版本将默认的推理引擎替换为了 IPEX-LLM，从而打通了软件与 Arc 显卡之间的加速通道。这并非一个脆弱或不稳定的社区项目，而是由硬件制造商官方支持的解决方案。英特尔通过其官方 GitHub 仓库积极开发并推广此方案，发布了易于使用的“便携式压缩包”（Portable Zips），并提供了详尽的快速入门文档，这为方案的可靠性和未来更新提供了有力保障 。  </p>\n<h3 id=\"1-3-我们的路线图\"><a href=\"#1-3-我们的路线图\" class=\"headerlink\" title=\"1.3. 我们的路线图\"></a>1.3. 我们的路线图</h3><p>本教程将遵循一条清晰的路径，引导您完成从零到一的全部署过程：</p>\n<ol>\n<li><strong>系统基础准备</strong>：安装并配置必要的驱动程序和软件环境。</li>\n<li><strong>核心引擎部署</strong>：安装并启动经过 IPEX-LLM 加速的 Ollama 后端服务。</li>\n<li><strong>前端界面搭建</strong>：安装并连接用户友好的 Open WebUI，提供图形化交互界面。</li>\n<li><strong>验证与排错</strong>：确认 GPU 加速已成功启用，并提供常见问题的解决方案。</li>\n</ol>\n<hr>\n<h2 id=\"第二部分：基础设置-准备您的-Windows-系统\"><a href=\"#第二部分：基础设置-准备您的-Windows-系统\" class=\"headerlink\" title=\"第二部分：基础设置 - 准备您的 Windows 系统\"></a>第二部分：基础设置 - 准备您的 Windows 系统</h2><p>在开始部署之前，确保系统环境符合要求是成功的先决条件。下面的清单和步骤将帮助您准备好一切。</p>\n<p><strong>系统必备组件清单</strong></p>\n<table>\n<thead>\n<tr>\n<th>组件</th>\n<th>推荐版本/要求</th>\n<th>下载地址</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>英特尔 Arc 显卡驱动</td>\n<td>最新 WHQL 认证版 (例如 32.0.101.6881 或更高)</td>\n<td><a href=\"https://www.intel.com/content/www/us/en/products/sku/229151/intel-arc-a770-graphics-16gb/downloads.html\">英特尔官方下载中心</a></td>\n</tr>\n<tr>\n<td>Miniforge (Conda)</td>\n<td>最新版</td>\n<td>(<a href=\"https://github.com/conda-forge/miniforge/releases/latest\">https://github.com/conda-forge/miniforge/releases/latest</a>)</td>\n</tr>\n<tr>\n<td>Windows Terminal</td>\n<td>最新版</td>\n<td>(<a href=\"https://apps.microsoft.com/store/detail/windows-terminal/9N0DX20HK701\">https://apps.microsoft.com/store/detail/windows-terminal/9N0DX20HK701</a>)</td>\n</tr>\n</tbody></table>\n<h3 id=\"2-1-英特尔-Arc-显卡驱动：性能的基石\"><a href=\"#2-1-英特尔-Arc-显卡驱动：性能的基石\" class=\"headerlink\" title=\"2.1. 英特尔 Arc 显卡驱动：性能的基石\"></a>2.1. 英特尔 Arc 显卡驱动：性能的基石</h3><p>整个技术栈依赖于最新的显卡驱动程序通过英特尔 oneAPI Level Zero 接口与 GPU 进行通信。过时、损坏或由 Windows Update 自动安装的通用驱动程序是导致失败的主要原因。</p>\n<p><strong>第一步：识别当前驱动版本</strong> 打开英特尔锐炫控制中心（Intel Arc Control）软件，或通过设备管理器查看当前安装的显卡驱动程序版本 。  </p>\n<p><strong>第二步：下载正确的驱动程序</strong> 访问上表中提供的英特尔官方下载中心链接，下载适用于 Arc A770 16GB 的最新 WHQL 认证驱动程序。根据资料，一个可靠的版本是 <code>32.0.101.6881</code>，对应的安装文件名为 <code>gfx_win_101.6881.exe</code> 。  </p>\n<p><strong>第三步：执行一次“清洁安装”（推荐的最佳实践）</strong> 为了从根源上避免潜在的、难以诊断的故障，强烈建议执行一次“清洁安装”。在驱动安装过程中，选择“自定义安装”，然后勾选“执行清洁安装”选项。这将移除所有旧的驱动文件和配置，避免新旧文件冲突。</p>\n<p>对于追求极致稳定或在后续步骤中遇到蓝屏错误（如 <code>VIDEO_SCHEDULER_INTERNAL_ERROR</code> ）的用户，可以采用更彻底的方法：使用 Display Driver Uninstaller (DDU) 工具。这是英特尔官方支持文档中也提及的高级方法，可以确保完全清除旧驱动的残留 。  </p>\n<p><strong>第四步：验证安装</strong> 安装完成后，重启计算机。再次打开英特尔锐炫控制中心，确认驱动程序版本已更新为最新版本。</p>\n<h3 id=\"2-2-命令行与环境设置\"><a href=\"#2-2-命令行与环境设置\" class=\"headerlink\" title=\"2.2. 命令行与环境设置\"></a>2.2. 命令行与环境设置</h3><p><strong>第一步：安装 Miniforge</strong> 相比于完整的 Anaconda 发行版，Miniforge 更为轻量，是搭建 Python 环境的更优选择。运行 Miniforge 安装程序，在安装过程中，建议勾选“Add Miniforge3 to my PATH environment variable”选项，这将简化后续的命令行操作 。  </p>\n<p><strong>第二步：安装与配置 Windows Terminal</strong> 从 Microsoft Store 安装 Windows Terminal，它提供了现代化的多标签页和强大的 PowerShell 支持，是执行后续所有命令的推荐工具。</p>\n<p><strong>第三步：为 PowerShell 初始化 Conda</strong> 这是一个常见的障碍点。打开 Windows Terminal (PowerShell)，运行以下命令：</p>\n<pre class=\"line-numbers language-PowerShell\" data-language=\"PowerShell\"><code class=\"language-PowerShell\">conda init powershell<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n\n<p>此命令会修改您的 PowerShell 配置文件，使其能够识别 <code>conda</code> 命令 。完成后，必须关闭并重新打开 Windows Terminal 才能使更改生效。  </p>\n<p><strong>第四步：设置 PowerShell 执行策略</strong> 出于安全考虑，PowerShell 的默认执行策略可能会阻止 Conda 激活脚本的运行。在新的 PowerShell 窗口中，运行以下命令以允许这些脚本执行：</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">Set-ExecutionPolicy RemoteSigned -Scope CurrentUser<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n\n<p>在提示时，输入 <code>Y</code> 并按回车确认 。  </p>\n<p><strong>第五步：验证 Conda 安装</strong> 在 PowerShell 口中输入 <code>conda --version</code>。如果安装正确，将显示 Conda 的版本号，表明您的命令行环境已准备就绪。</p>\n<hr>\n<h2 id=\"第三部分：核心引擎-部署加速的-Ollama-后端\"><a href=\"#第三部分：核心引擎-部署加速的-Ollama-后端\" class=\"headerlink\" title=\"第三部分：核心引擎 - 部署加速的 Ollama 后端\"></a>第三部分：核心引擎 - 部署加速的 Ollama 后端</h2><h3 id=\"3-1-为何选择-IPEX-LLM-定制版-Ollama\"><a href=\"#3-1-为何选择-IPEX-LLM-定制版-Ollama\" class=\"headerlink\" title=\"3.1. 为何选择 IPEX-LLM 定制版 Ollama\"></a>3.1. 为何选择 IPEX-LLM 定制版 Ollama</h3><p>在此必须明确：从 <code>ollama.com</code> 官方网站下载的标准版 Ollama 安装程序 <strong>无法</strong> 用于本教程的目标。如果您已经安装了标准版，请先将其卸载。</p>\n<p>我们将使用的是英特尔提供的、内建了 IPEX-LLM 支持的特殊版本。它通过 oneAPI 和 SYCL 技术栈，将推理任务直接交由英特尔 GPU 处理，从而实现硬件加速 。英特尔提供的“便携式压缩包”方案，极大地降低了用户的配置门槛，是目前最推荐、最便捷的部署方式，避免了早期版本中复杂的依赖安装和初始化脚本步骤 。  </p>\n<h3 id=\"3-2-创建专用的-AI-后端环境\"><a href=\"#3-2-创建专用的-AI-后端环境\" class=\"headerlink\" title=\"3.2. 创建专用的 AI 后端环境\"></a>3.2. 创建专用的 AI 后端环境</h3><p>为了避免不同软件间的依赖冲突，我们将为 Ollama 后端创建一个独立的、干净的 Conda 环境。</p>\n<ol>\n<li><p>在 PowerShell 中，运行以下命令创建环境：</p>\n<pre class=\"line-numbers language-PowerShell\" data-language=\"PowerShell\"><code class=\"language-PowerShell\">conda create -n ipex-ollama python=3.11 -y<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n\n<p>这会创建一个名为 <code>ipex-ollama</code> 的环境，并指定 Python 版本为 3.11，这是经过验证的兼容版本 。  </p>\n</li>\n<li><p>激活该环境：</p>\n<pre class=\"line-numbers language-PowerShell\" data-language=\"PowerShell\"><code class=\"language-PowerShell\">conda activate ipex-ollama<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n\n<p>激活后，PowerShell 提示符前会显示 <code>(ipex-ollama)</code>，表示您当前正工作在此环境中。</p>\n</li>\n</ol>\n<h3 id=\"3-3-安装-IPEX-LLM-版-Ollama\"><a href=\"#3-3-安装-IPEX-LLM-版-Ollama\" class=\"headerlink\" title=\"3.3. 安装 IPEX-LLM 版 Ollama\"></a>3.3. 安装 IPEX-LLM 版 Ollama</h3><p><strong>第一步：下载便携式软件包</strong> 访问(<a href=\"https://github.com/intel/ipex-llm/releases)%EF%BC%8C%E6%89%BE%E5%88%B0%E6%9C%80%E6%96%B0%E7%9A%84%E5%8F%91%E8%A1%8C%E7%89%88%EF%BC%8C%E5%B9%B6%E4%B8%8B%E8%BD%BD%E9%80%82%E7%94%A8%E4%BA%8E\">https://github.com/intel/ipex-llm/releases)，找到最新的发行版，并下载适用于</a> Windows 的 <code>Ollama-Portable-Zip</code> 文件 。  </p>\n<p><strong>第二步：解压软件包</strong> 在您的硬盘上创建一个专用于 AI 工具的文件夹，例如 <code>C:\\ai-tools\\</code>。然后在此文件夹下再创建一个 <code>ipex-ollama</code> 文件夹，并将下载的压缩包内容完整地解压到这里。最终路径应类似于 <code>C:\\ai-tools\\ipex-ollama</code>。</p>\n<p><strong>第三步：在终端中导航至该目录</strong> 在已激活 <code>ipex-ollama</code> 环境的 PowerShell 窗口中，使用 <code>cd</code> 命令进入您刚刚创建的文件夹：</p>\n<pre class=\"line-numbers language-PowerShell\" data-language=\"PowerShell\"><code class=\"language-PowerShell\">cd C:\\ai-tools\\ipex-ollama<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n\n<h3 id=\"3-4-启动-GPU-加速的-Ollama-服务器\"><a href=\"#3-4-启动-GPU-加速的-Ollama-服务器\" class=\"headerlink\" title=\"3.4. 启动 GPU 加速的 Ollama 服务器\"></a>3.4. 启动 GPU 加速的 Ollama 服务器</h3><p>这是整个流程中最关键的一步。我们需要通过设置特定的环境变量来“指挥”Ollama 使用 GPU。</p>\n<p><strong>关键环境变量解析</strong></p>\n<table>\n<thead>\n<tr>\n<th>环境变量</th>\n<th>推荐值</th>\n<th>目的与解释</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>OLLAMA_NUM_GPU</code></td>\n<td><code>999</code></td>\n<td><strong>最核心的设置</strong>。此变量告知 Ollama 将模型的多少层卸载到 GPU。设置为一个非常大的数字（如 999）等同于“全部卸载”，以最大化利用 GPU 资源 。</td>\n</tr>\n<tr>\n<td><code>ZES_ENABLE_SYSMAN</code></td>\n<td><code>1</code></td>\n<td>启用 Level Zero 驱动程序栈中的 Sysman 库。IPEX-LLM 需要此库来进行 GPU 监控和内存管理 。</td>\n</tr>\n<tr>\n<td><code>SYCL_CACHE_PERSISTENT</code></td>\n<td><code>1</code></td>\n<td>启用持久化 SYCL 内核缓存。当模型第一次加载时，SYCL 会将编译后的 GPU 内核代码缓存到硬盘。这会显著加快后续加载相同或不同模型的速度 。</td>\n</tr>\n</tbody></table>\n<p><strong>启动服务器的命令</strong></p>\n<ol>\n<li><p>在 PowerShell 中，逐行运行以下命令来设置环境变量。这些设置仅对当前终端会话有效。</p>\n<pre class=\"line-numbers language-PowerShell\" data-language=\"PowerShell\"><code class=\"language-PowerShell\">$env:OLLAMA_NUM_GPU=\"999\"\n$env:ZES_ENABLE_SYSMAN=\"1\"\n$env:SYCL_CACHE_PERSISTENT=\"1\"<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n</li>\n<li><p>设置完毕后，运行以下命令启动 Ollama 服务器：</p>\n<pre class=\"line-numbers language-PowerShell\" data-language=\"PowerShell\"><code class=\"language-PowerShell\">.\\ollama serve\nor\nstart-ollama.bat<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre></li>\n</ol>\n<p>服务器成功启动后，您会看到类似 <code>time=... level=INFO source=server.go:.. msg=\"starting server...\"</code> 的日志输出。<strong>请务必保持此终端窗口处于打开状态</strong>，因为它就是您的 AI 推理引擎。</p>\n<hr>\n<h2 id=\"第四部分：用户体验-安装并连接-Open-WebUI\"><a href=\"#第四部分：用户体验-安装并连接-Open-WebUI\" class=\"headerlink\" title=\"第四部分：用户体验 - 安装并连接 Open WebUI\"></a>第四部分：用户体验 - 安装并连接 Open WebUI</h2><p>Ollama 后端已经运行，但它只提供了一个命令行接口。为了获得类似 ChatGPT 的图形化聊天体验，我们需要安装一个前端界面——Open WebUI。</p>\n<h3 id=\"4-1-准备前端环境\"><a href=\"#4-1-准备前端环境\" class=\"headerlink\" title=\"4.1. 准备前端环境\"></a>4.1. 准备前端环境</h3><p>同样地，我们将为 Open WebUI 创建一个独立的环境，以确保其依赖项不会与后端环境发生冲突。</p>\n<ol>\n<li><p><strong>打开一个新的 Windows Terminal 标签页或窗口</strong>。不要在运行着 Ollama 服务器的窗口中操作。</p>\n</li>\n<li><p>创建新的 Conda 环境：</p>\n<pre class=\"line-numbers language-PowerShell\" data-language=\"PowerShell\"><code class=\"language-PowerShell\">conda create -n open-webui python=3.11 -y<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n</li>\n<li><p>激活此新环境：</p>\n<pre class=\"line-numbers language-PowerShell\" data-language=\"PowerShell\"><code class=\"language-PowerShell\">conda activate open-webui<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre></li>\n</ol>\n<h3 id=\"4-2-安装并启动-Web-界面\"><a href=\"#4-2-安装并启动-Web-界面\" class=\"headerlink\" title=\"4.2. 安装并启动 Web 界面\"></a>4.2. 安装并启动 Web 界面</h3><ol>\n<li><p>在已激活 <code>open-webui</code> 环境的 PowerShell 窗口中，使用 <code>pip</code> 命令安装 Open WebUI：</p>\n<pre class=\"line-numbers language-PowerShell\" data-language=\"PowerShell\"><code class=\"language-PowerShell\">pip install open-webui<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n</li>\n<li><p>安装完成后，运行以下命令启动其 Web 服务器：</p>\n<pre class=\"line-numbers language-PowerShell\" data-language=\"PowerShell\"><code class=\"language-PowerShell\">open-webui serve<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre></li>\n</ol>\n<p>服务器启动后，您会看到日志输出，并提示服务正在 <code>http://localhost:8080</code> 上运行 。  </p>\n<p><strong>同样，这个终端窗口也必须保持打开状态</strong>。至此，您应该有两个持续运行的终端窗口：一个用于 Ollama 后端，一个用于 Open WebUI 前端。</p>\n<h3 id=\"4-3-首次设置与连接\"><a href=\"#4-3-首次设置与连接\" class=\"headerlink\" title=\"4.3. 首次设置与连接\"></a>4.3. 首次设置与连接</h3><ol>\n<li>打开您的网页浏览器（如 Chrome, Edge, Firefox），访问地址：<code>http://localhost:8080</code>。</li>\n<li>首次访问时，Open WebUI 会引导您创建一个管理员账户。请按提示完成注册 。  </li>\n<li>登录后，Open WebUI 通常会自动检测到在本地 <code>http://localhost:11434</code> 端口上运行的 Ollama 服务并建立连接 。  </li>\n<li>如果连接失败或未自动识别，可以手动进行配置。点击左侧边栏的“设置”图标，进入“连接”选项卡，确认 Ollama API 的 URL 地址已正确设置为 <code>http://localhost:11434</code> 。</li>\n</ol>\n<hr>\n<h2 id=\"第五部分：验证与首次使用-对系统进行测试\"><a href=\"#第五部分：验证与首次使用-对系统进行测试\" class=\"headerlink\" title=\"第五部分：验证与首次使用 - 对系统进行测试\"></a>第五部分：验证与首次使用 - 对系统进行测试</h2><p>现在，万事俱备，是时候验证我们的成果并运行第一个大语言模型了。</p>\n<h3 id=\"5-1-运行您的第一个-LLM\"><a href=\"#5-1-运行您的第一个-LLM\" class=\"headerlink\" title=\"5.1. 运行您的第一个 LLM\"></a>5.1. 运行您的第一个 LLM</h3><ol>\n<li>在 Open WebUI 的主界面，点击顶部的模型选择框。</li>\n<li>在输入框中键入一个模型名称，例如 <code>phi3:medium</code> 或 <code>llama3:8b</code>。由于模型尚未在本地安装，WebUI 会提示您下载它。点击确认后，Open WebUI 会向后端发送指令，开始下载模型 。  </li>\n<li>此时，您可以切换到运行着 <code>ollama serve</code> 的终端窗口，观察模型的下载进度条。</li>\n<li>模型下载完成后，在 WebUI 的聊天框中输入一个简单的问题，例如“天空为什么是蓝色的？”，然后按回车。片刻之后，您应该就能看到由您的 Arc A770 显卡驱动生成的回答。</li>\n</ol>\n<h3 id=\"5-2-确认真正的-GPU-加速\"><a href=\"#5-2-确认真正的-GPU-加速\" class=\"headerlink\" title=\"5.2. 确认真正的 GPU 加速\"></a>5.2. 确认真正的 GPU 加速</h3><p>如何确定模型确实是由 GPU 而非 CPU 运行的？Windows 任务管理器可以给我们明确的答案。</p>\n<p><strong>第一步：打开任务管理器</strong> 按下 <code>Ctrl+Shift+Esc</code> 组合键。</p>\n<p><strong>第二步：导航至“性能”选项卡</strong> 在任务管理器窗口中，点击“性能”选项卡，然后在左侧边栏中找到并点击您的“Intel Arc(TM) Graphics” GPU。</p>\n<p><strong>第三步：更改引擎视图（关键步骤）</strong> 任务管理器默认显示的 GPU 利用率图表是“3D”引擎，这主要用于游戏和图形渲染，与我们的计算任务无关。您需要点击图表上方的下拉菜单（默认为“3D”），然后将其更改为 <strong>Compute_0</strong> 或 <strong>Graphics_1</strong> 。这是 DirectML 和 SYCL 等计算 API 使用的引擎。  </p>\n<p><strong>第四步：观察利用率</strong> 回到 Open WebUI，再次向模型提问，并密切关注任务管理器。在模型生成回答的几秒钟内，您应该能看到“Compute_0”或“Graphics_1”图表出现明显的、尖锐的峰值。</p>\n<p><strong>第五步：观察显存（VRAM）使用情况</strong> 在同一性能页面下，找到“专用 GPU 内存”图表。当您加载并运行模型时，该图表的占用率会显著上升，并维持在一个较高的水平，这表明模型权重已被成功加载到显卡的 VRAM 中 。  </p>\n<p><strong>计算引擎的活动和专用显存的高占用率，是 GPU 加速已成功启用的确凿证据</strong>。</p>\n<h3 id=\"5-3-性能健全性检查：CPU-vs-GPU-对比\"><a href=\"#5-3-性能健全性检查：CPU-vs-GPU-对比\" class=\"headerlink\" title=\"5.3. 性能健全性检查：CPU vs. GPU 对比\"></a>5.3. 性能健全性检查：CPU vs. GPU 对比</h3><p>为了更直观地感受性能差异，可以进行一个简单的对比测试。</p>\n<ol>\n<li>在运行着 <code>ollama serve</code> 的终端窗口中，按 <code>Ctrl+C</code> 停止服务器。</li>\n<li><strong>不设置</strong> <code>OLLAMA_NUM_GPU</code> 环境变量，直接运行 <code>.\\ollama serve</code> 启动服务器。这将强制 Ollama 使用 CPU 进行推理。</li>\n<li>回到 Open WebUI，使用相同的模型提出相同的问题。您会发现回答的生成速度（以 tokens/秒计）明显变慢。</li>\n<li>同时观察任务管理器，此时您会看到 CPU 利用率飙升，而 GPU 的计算引擎则处于空闲状态。这个鲜明的对比无可辩驳地证明了我们之前配置的价值 。</li>\n</ol>\n<hr>\n<h2 id=\"第六部分：深度排错与高级主题\"><a href=\"#第六部分：深度排错与高级主题\" class=\"headerlink\" title=\"第六部分：深度排错与高级主题\"></a>第六部分：深度排错与高级主题</h2><h3 id=\"6-1-常见问题与解决方案\"><a href=\"#6-1-常见问题与解决方案\" class=\"headerlink\" title=\"6.1. 常见问题与解决方案\"></a>6.1. 常见问题与解决方案</h3><table>\n<thead>\n<tr>\n<th>症状</th>\n<th>可能原因</th>\n<th>解决方案</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>加载模型时出现 <code>VIDEO_SCHEDULER_INTERNAL_ERROR</code> 蓝屏死机 (BSOD)</td>\n<td>显卡驱动程序不稳定或存在冲突。</td>\n<td>按照第二部分 2.1 节的指导，执行一次驱动程序的“清洁安装”。如果问题依旧，使用 DDU 工具彻底卸载后重装 。</td>\n</tr>\n<tr>\n<td>模型运行缓慢，任务管理器显示 CPU 占用高，GPU 计算引擎无活动。</td>\n<td>启动 <code>ollama serve</code> 之前，未正确设置 <code>$env:OLLAMA_NUM_GPU=\"999\"</code> 环境变量。</td>\n<td>停止服务器 (<code>Ctrl+C</code>)，在 PowerShell 中正确设置该环境变量后，再重新启动服务器 。</td>\n</tr>\n<tr>\n<td>PowerShell 中提示 <code>conda activate</code> 命令未找到。</td>\n<td>Conda 未针对 PowerShell 进行初始化。</td>\n<td>运行 <code>conda init powershell</code> 命令，然后关闭并重新打开 Windows Terminal 。</td>\n</tr>\n<tr>\n<td>Open WebUI 界面显示“服务器连接错误”。</td>\n<td>Ollama 后端服务器未运行，或网络配置问题。</td>\n<td>确保运行 <code>ollama serve</code> 的终端窗口没有关闭且无错误。在 Open WebUI 的“设置 &gt; 连接”中，检查 API URL 是否为 <code>http://localhost:11434</code> 。</td>\n</tr>\n<tr>\n<td>Conda 安装 (<code>pip install</code> 或 <code>conda create</code>) 失败或卡在“Solving environment”。</td>\n<td>Conda 频道冲突或网络连接问题。</td>\n<td>确保网络连接稳定。尝试在终端运行 <code>conda clean -a -y</code> 清理缓存。作为最后的手段，可以尝试 <code>conda config --set channel_priority flexible</code> 。</td>\n</tr>\n</tbody></table>\n<h3 id=\"6-2-定位与阅读日志文件\"><a href=\"#6-2-定位与阅读日志文件\" class=\"headerlink\" title=\"6.2. 定位与阅读日志文件\"></a>6.2. 定位与阅读日志文件</h3><p>当遇到更复杂的问题时，日志文件是您的最佳帮手。</p>\n<ul>\n<li><p><strong>日志文件位置</strong>：</p>\n<ul>\n<li><p><strong>服务器日志</strong>：<code>%LOCALAPPDATA%\\Ollama</code> 目录下的 <code>server.log</code> 文件包含了最新的服务器运行日志 。  </p>\n</li>\n<li><p><strong>模型存储位置</strong>：<code>%HOMEPATH%\\.ollama</code> 目录，这里存放着您下载的所有模型文件 。  </p>\n<p>您可以在文件资源管理器的地址栏直接输入以上路径（包括百分号）来快速访问。</p>\n</li>\n</ul>\n</li>\n<li><p><strong>启用调试日志</strong>： 为了获得更详细的排错信息，可以停止 Ollama 服务器，然后使用以下命令重启，这将启用调试模式：</p>\n<pre class=\"line-numbers language-PowerShell\" data-language=\"PowerShell\"><code class=\"language-PowerShell\">$env:OLLAMA_DEBUG=\"1\"<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre></li>\n</ul>\n<h3 id=\"6-3-高级自定义：更改模型存储位置\"><a href=\"#6-3-高级自定义：更改模型存储位置\" class=\"headerlink\" title=\"6.3. 高级自定义：更改模型存储位置\"></a>6.3. 高级自定义：更改模型存储位置</h3><p>LLM 模型文件通常体积巨大，很多用户希望将它们存储在非系统盘（如 D 盘）上。</p>\n<ol>\n<li>打开 Windows 的“高级系统设置”（可以在开始菜单中搜索）。</li>\n<li>点击“环境变量”按钮。</li>\n<li>在“用户变量”区域，点击“新建”。</li>\n<li>变量名填写 <code>OLLAMA_MODELS</code>。</li>\n<li>变量值填写您希望存储模型的新路径，例如 <code>D:\\OllamaModels</code>。</li>\n<li>逐级点击“确定”保存设置。</li>\n<li><strong>此更改需要完全退出并重新启动 Ollama 应用程序（包括后台服务）才能生效</strong> 。</li>\n</ol>\n<p>通过遵循本指南的详尽步骤，您已成功将您的英特尔 Arc A770 16G 显卡打造成一个强大的本地 AI 推理平台。享受探索大型语言模型世界的乐趣吧！</p>\n",
            "tags": [
                "AI"
            ]
        }
    ]
}